{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Temple's Data Analytics Full-Time Program\n",
    "---\n",
    "## Python for Data Analysis: Machine Learning in Python using Scikit-Learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with the `scikit-learn` library, we will need to pip install it in our local machine. We will also want to pip install another couple of libraries called: `category encoders` and `xgboost`.\n",
    "\n",
    "While sklearn has built-in functionality for encoders such as OHE and OrdinalEncoder, the output of these encoders are different than that of the `category encoders` library. We will work with both libraries in conjunction together to make a pipeline object.\n",
    "\n",
    "The library `xgboost` will allow us access to our boosted models for both Regression and Classification. We will cover over boosting and other ensemble methods later in this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.26.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category_encoders in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.6.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (1.26.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (1.3.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (1.11.3)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (0.14.0)\n",
      "Requirement already satisfied: pandas>=1.0.5 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (2.1.1)\n",
      "Requirement already satisfied: patsy>=0.5.1 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from category_encoders) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nargi\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.0.5->category_encoders) (2023.3)\n",
      "Requirement already satisfied: six in c:\\users\\nargi\\appdata\\roaming\\python\\python311\\site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.20.0->category_encoders) (3.2.0)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\nargi\\appdata\\roaming\\python\\python311\\site-packages (from statsmodels>=0.9.0->category_encoders) (23.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.26.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\nargi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from xgboost) (1.11.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install scikit-learn\n",
    "!python -m pip install category_encoders\n",
    "!python -m pip install xgboost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when it comes to the actual imports of each item we are going to use today, we will be accessing many different parts of the sklearn library. To see all the available parts of sklearn, check out this [documentation](https://scikit-learn.org/stable/auto_examples/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: EDA\n",
    "In this part, we will focus on reading in the data and performing our exploratory data analysis of the dataset.\n",
    "Our overall goal is to be able to predict the price of a house based off the features given to us in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>house_id</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>style</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1112</td>\n",
       "      <td>B</td>\n",
       "      <td>1188</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>598291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>491</td>\n",
       "      <td>B</td>\n",
       "      <td>3512</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1744259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5952</td>\n",
       "      <td>B</td>\n",
       "      <td>1134</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>571669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3525</td>\n",
       "      <td>A</td>\n",
       "      <td>1940</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>ranch</td>\n",
       "      <td>493675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5108</td>\n",
       "      <td>B</td>\n",
       "      <td>2208</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>victorian</td>\n",
       "      <td>1101539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   house_id neighborhood  area  bedrooms  bathrooms      style    price\n",
       "0      1112            B  1188         3          2      ranch   598291\n",
       "1       491            B  3512         5          3  victorian  1744259\n",
       "2      5952            B  1134         3          2      ranch   571669\n",
       "3      3525            A  1940         4          2      ranch   493675\n",
       "4      5108            B  2208         6          4  victorian  1101539"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data using pandas\n",
    "df = pd.read_csv(r'C:\\Users\\nargi\\Documents\\coding temple\\data\\house prices.csv')\n",
    "\n",
    "# View the dataset\n",
    "df.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What needs to be done to this dataset?\n",
    "\n",
    "* Are all columns the data type they need to be?\n",
    "* Count of null values\n",
    "* Visualization of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are all the columns the proper type and how many null values are we dealing with?\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that we have 0 null values present within this dataset. Each feature in our dataset is the appropriate data-type as well. There is no cleaning that needs to be done to this data before we begin working with it. Let's double-check our distribution of the target variable as well as a scatterplot of it against each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's graph out a histogram of our target variable\n",
    "plt.hist('price', data=df);\n",
    "plt.xlabel('Price by Hundred-Thousand');\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we notice about the distribution of our target variable? \n",
    "\n",
    "How does the skewness of the target affect a model's ability to make predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's visualize the relationship between our target and the rest of our features\n",
    "sns.pairplot(df, y_vars=['price'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we notice about the relationships between each of these variables and the price column?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise 1: Try it out\n",
    "For this exercise, you will download the data from [Kaggle](https://www.kaggle.com/datasets/rajkumarpandey02/vehicle-fuel-economy-data-us). Once downloaded, your goal will be to:\n",
    "\n",
    "* Load in the data using Pandas\n",
    "* Clean the data (check the data-types for inconsistencies, check for null values, change column names if needed)\n",
    "* Identify and create a visualization of what you believe the target should be\n",
    "* Create a pairplot of the target to the rest of your features present within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Separating the target and splitting the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with a machine learning model, it is important to understand how to present the data to that model in order to make an accurate prediction. To start with, there are two things we need to accomplish before moving forward:\n",
    "\n",
    "1. We must remove the target vector from the feature matrix. If we leave in the answers to the test before giving it to the model, it cheats and can get a perfect score on the training, but perform terribly outside of the training environment and on any new data. This is also known as data leakage, which is a cause of what we call an overfit model. \n",
    "\n",
    "2. We must also split the data apart into a training and testing set. In order to test the effectiveness of a model, we will typically hold back some of our data in order to test how the model will perform when faced with data it has never seen before. The amount of this split is typically 80/20; 80% of the data goes into the training dataset, while the reserved 20% is used as a testing set! We want to include as much data as possible in the training set so our model can draw the best conclusions it can about the target and create more accurate predictions. This will also aid you in finding any overfitting issues your model may be having."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify the target vector\n",
    "target = 'price'\n",
    "\n",
    "# Step 2: Separate the target vector from the feature matrix\n",
    "X = df.drop(columns=target)\n",
    "y = df[target]\n",
    "\n",
    "# Step 3: Split the data into a training and testing set, using sklearn\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, train_size=.8, random_state=42)\n",
    "\n",
    "# View our testing data\n",
    "display(x_test)\n",
    "print(y_test)\n",
    "\n",
    "# Assert statement to double-check our work:\n",
    "assert len(x_train) + len(x_test) == len(X), 'You did not separate the data properly'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Establish a baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `baseline` is nothing more than a way to check our model's performance. It is the simplest prediction we can make about the data. \n",
    "\n",
    "For regression analysis, this baseline is the mean of our target vector. This would be the same as always predicting the average value for each price data-point in the target.\n",
    "\n",
    "For classification analysis, the baseline is the majority class and the percentage that it occurs becomes your baseline value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a baseline:\n",
    "baseline = [df.price.mean()] * len(df)\n",
    "baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the average value, we would predict $754,207.88 as the price for each datapoint."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Model Building\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lecture, we will be walking through multiple models and some new concepts. It can be a lot to absorb all at once, so here is some explanations:\n",
    "\n",
    "In order to place the data into a model, we will need to preprocess the data. This preprocessing step usually involves two pieces:\n",
    "\n",
    "1. **An Encoder**: In order to work with string-type data, we need to change the data to a format our model can understand. In this lecture, we will focus on two different types of encoders;\n",
    "    \n",
    "    i. **OneHotEncoder**: OneHotEncoding, or also known as OHE, transforms a string column into multiple columns. These columns are labeled with each unique value in the dataset and is a boolean data-type. Each value in each column corresponds to whether that value was present within the original column in that row(1) or not(0). This can be useful for a dataset with only one or two string columns and only a few unique values present in them. OHE does run into a dimensionality issue when you begin to plug in variables with high cardinality(a lot of unique values) or a lot of different string columns. \n",
    "\n",
    "    ii. **OrdinalEncoder**: Ordinal Encoding changes the actual values themselves into a numeric representation of the string present within the column. This solves any issues regarding high cardinality, but can result in the loss of some information between the model and the data.\n",
    "    \n",
    "Overall, it is important to choose the right encoder for the job in order to provide your model with the best chance at making it's predictions!\n",
    "\n",
    "2. **An Imputer**: A lot of models don't know how to handle missing values, and missing values can also lead to bias in the model if it does accept them. To circumvent this, we use an imputer. An imputer will infer information about a column, such as the basic statitstics of the column, and fill the missing values based on the method specified. When left blank, the method will default to the mean of the column! We will utilize the `SimpleImputer` module of `sklearn` to accomplish this today. As our current dataset does not have any null values, we will not include one in our pipeline. We will see the use of this later on in this notebook!\n",
    "\n",
    "Now that we understand the preprocessing of our data, we can begin building our model! Here are the steps we will need to take:\n",
    "\n",
    "1. We will be creating a pipeline object that combines all of our steps together into a singular object. This will save us a lot of work and time. In order to do so, we will access the `make_pipeline` module of `sklearn`.\n",
    "\n",
    "2. Inside the pipeline object, we will instantiate all preprocessors and the model!\n",
    "\n",
    "3. After creation, we will need to fit the model to our training data. **DO NOT FIT THE MODEL TO TESTING DATA!!!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a look at all the steps we would need to go through in order to create a model without a pipeline object and the formula that is behind our Linear Regression model we will be using:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://editor.analyticsvidhya.com/uploads/375512.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fig-lin-reg.webp](data:image/webp;base64,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)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><strong>Intercept (<em>b</em><sub>0</sub>)</strong> - Expected value of response variable (y) when explanatory variable (x) is 0</p>\n",
    "<p><strong>Slope (<em>b</em><sub>1</sub>)</strong> - Expected value of response variable (y) when explanatory variable (x) is 0</p>\n",
    "<p><strong>Best fit - <em>y</em> = <em>b</em><sub>0</sub> + <em>b</em><sub>1</sub><em>x</em></strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Instantiate our Encoder\n",
    "ohe = OneHotEncoder(use_cat_names=True)\n",
    "\n",
    "# Step 2: Fit the Encoder\n",
    "ohe.fit(x_train, y_train)\n",
    "\n",
    "# Step 3: Transform our data\n",
    "x_trans = ohe.transform(x_train)\n",
    "\n",
    "# Step 4: Instantiate the model object\n",
    "model_dt = LinearRegression()\n",
    "\n",
    "# Step 5: Fit the model object\n",
    "model_dt.fit(x_trans, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at how much easier our make_pipeline function makes this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1a: Create the pipeline object\n",
    "model_lr = make_pipeline(\n",
    "    # Step 1b: Add in any preprocessors to the pipeline\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    # Step 1c: Add in the model object to the pipeline\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "# Step 2: Fit the pipeline\n",
    "model_lr.fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fit a single model, let's take a look at other models we could use for this task and compare how each of them performs:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.saedsayad.com/images/Decision_tree_r1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1a: Create the pipeline object\n",
    "pipe_dt = make_pipeline(\n",
    "    # Step 1b: Add in any preprocessors to the pipeline\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    # Step 1c: Add in the model object to the pipeline\n",
    "    DecisionTreeRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Step 2: Fit the pipeline\n",
    "pipe_dt.fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/v2/resize:fit:1400/1*ZFuMI_HrI3jt2Wlay73IUQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1a: Create the pipeline object\n",
    "model_rf = make_pipeline(\n",
    "    # Step 1b: Add in any preprocessors to the pipeline\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    # Step 1c: Add in the model object to the pipeline\n",
    "    RandomForestRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Step 2: Fit the pipeline\n",
    "model_rf.fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/xgboost_illustration.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1a: Create the pipeline object\n",
    "model_xg = make_pipeline(\n",
    "    # Step 1b: Add in any preprocessors to the pipeline\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    # Step 1c: Add in the model object to the pipeline\n",
    "    XGBRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Step 2: Fit the pipeline\n",
    "model_xg.fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise #2: Build your own models\n",
    "\n",
    "For this in-class exercise, you will use the dataset you cleaned in the previous task to create 4 different model objects. Remember that you will need to split the data BEFORE creating/fitting each model. For one of the model objects, create it outside of a pipeline with all necessary preprocessors. For the rest, create a pipeline object with all necessary preprocessors and the model object inside of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 1 HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 2 HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 3 HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL 4 HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Check Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we built out all these models and fit them to the datasets. Now what? Well, we need to check on the accuracy of these models and compare each to our baseline model to see if it is performing better or worse than the baseline! To do so, we will utilize a few different modules. Note that the measurements of accuracy differ between a Regression analysis and a Classification analysis. In regression, we will use:\n",
    "\n",
    "* **mean absolute error(MAE)**: Calculated using the actual and predicted values. This measures the residuals of the model's predictions from the actual values and returns the mean value of those residuals\n",
    "\n",
    "* **mean squared error(MSE)**:Along the same lines as MAE, however, MSE squares the predicted and actual value before subtraction and division, causing errors to be very obvious. MSE can show large numbers due to the amount of variance actually present within the model.\n",
    "\n",
    "* **r2 score**: Overall assessment of model accuracy. The values range from 0 to 1, indicating a percentage of data that the model is correctly predicting.\n",
    "\n",
    "Remember that there is no one sure-fire way to check a model's accuracy. The culmination of all these should be what affects our decision on a model, not the value of a singular metric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree Scores:')\n",
    "print(mean_absolute_error(y_train, pipe_dt.predict(x_train)))\n",
    "print(mean_absolute_error(y_test, pipe_dt.predict(x_test)))\n",
    "print('------------------------------')\n",
    "print('Linear Model Score:')\n",
    "print(mean_absolute_error(y_train, model_lr.predict(x_train)))\n",
    "print(mean_absolute_error(y_test, model_lr.predict(x_test)))\n",
    "print('---------------------------------------')\n",
    "print('Random Forest Score:')\n",
    "print(mean_absolute_error(y_train, model_rf.predict(x_train)))\n",
    "print(mean_absolute_error(y_test, model_rf.predict(x_test)))\n",
    "print('---------------------------------------')\n",
    "print('XGBoost Score:')\n",
    "print(mean_absolute_error(y_train, model_xg.predict(x_train)))\n",
    "print(mean_absolute_error(y_test, model_xg.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree Scores:')\n",
    "print(mean_squared_error(y_train, pipe_dt.predict(x_train)))\n",
    "print(mean_squared_error(y_test, pipe_dt.predict(x_test)))\n",
    "print('------------------------------')\n",
    "print('Linear Model Score:')\n",
    "print(mean_squared_error(y_train, model_lr.predict(x_train)))\n",
    "print(mean_squared_error(y_test, model_lr.predict(x_test)))\n",
    "print('---------------------------------------')\n",
    "print('Random Forest Score:')\n",
    "print(mean_squared_error(y_train, model_rf.predict(x_train)))\n",
    "print(mean_squared_error(y_test, model_rf.predict(x_test)))\n",
    "print('---------------------------------------')\n",
    "print('XGBoost Score:')\n",
    "print(mean_squared_error(y_train, model_xg.predict(x_train)))\n",
    "print(mean_squared_error(y_test, model_xg.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Decision Tree Scores:')\n",
    "print(r2_score(y_train, pipe_dt.predict(x_train)))\n",
    "print(r2_score(y_test, pipe_dt.predict(x_test)))\n",
    "print('------------------------------')\n",
    "print('Linear Model Score:')\n",
    "print(r2_score(y_train, model_lr.predict(x_train)))\n",
    "print(r2_score(y_test, model_lr.predict(x_test)))\n",
    "print('---------------------------------------')\n",
    "print('Random Forest Score:')\n",
    "print(r2_score(y_train, model_rf.predict(x_train)))\n",
    "print(r2_score(y_test, model_rf.predict(x_test)))\n",
    "print('---------------------------------------')\n",
    "print('XGBoost Score:')\n",
    "print(r2_score(y_train, model_xg.predict(x_train)))\n",
    "print(r2_score(y_test, model_xg.predict(x_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Tuning a model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we determine which model we want to use and will best fit our situation, we will want to tune the hyperparameters of that model. Hyperparameters refer to the parameters, or arguments, that each model takes. Each model has different parameters that can be tuned and each one has a level of significance to the final outcome of a model's prediction! Now, we could do this all by hand and input each of these values separately in the model, fit it, then check the metrics. However, there is an easier way to accomplish this task! By using the `RandomizedSearchCV` module, we will be able to parse over a parameter grid, or a dictionary where the key is the parameter and the value is all possible values you want to test the parameter at!\n",
    "\n",
    "Today, we will be looking over a few of these hyperparameters for the `RandomForestRegressor` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a parameter grid\n",
    "param_grid = {\n",
    "    'randomforestregressor__n_estimators': range(200,400, 10),\n",
    "    'randomforestregressor__max_depth' : range(10,110, 10),\n",
    "    'randomforestregressor__min_samples_split' : [2,5,10],\n",
    "    'randomforestregressor__min_samples_leaf' : [1,2,5]\n",
    "}\n",
    "\n",
    "# Step 2: Instantiate the RSCV module with the model object and the param_grid object\n",
    "clf = RandomizedSearchCV(model_rf, param_grid)\n",
    "\n",
    "# Step 3: Fit the data\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have this object, what can we do with it? We can pull the best parameters and best score from the object! Once we have this information, we can easily change these values during the instantiation of our model object and verify using our MSE and R2 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the best parameters for the model\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the best score the model could come up with\n",
    "clf.best_score_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-Class Exercise 3: \n",
    "For this exercise, your goal is to check the metrics of each of your model objects created in the last exercise. After you have identified the model you believe to work the best with the data, you will use the `RandomizedSearchCV` module to parse over a parameter grid and hypertune your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Classification Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = pd.read_csv(r'C:\\Users\\Alex Lucchesi\\coding-temple\\coding_temple_data_analytics_ft\\week-3\\data\\iris.csv')\n",
    "df_class.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Clean the data/EDA\n",
    "We will start off with tidying our data and completing our Exploratory Data Analysis\n",
    "\n",
    "* Are all columns the proper data-type?\n",
    "* What is the distribution of our target vector?\n",
    "* Are all the column names following proper conventions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start off with the column names:\n",
    "print(df_class.columns)\n",
    "\n",
    "# One method: removes the row that was in the column names though\n",
    "df_class.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "display(df_class.head())\n",
    "\n",
    "# Another method which doesn't cause us to lose any info:\n",
    "df_class = pd.read_csv(r'C:\\Users\\Alex Lucchesi\\coding-temple\\coding_temple_data_analytics_ft\\week-3\\data\\iris.csv',\n",
    "                       names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "\n",
    "display(df_class.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's check the data types and null-counts:\n",
    "df_class.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like each of our columns are the proper types and we have changed each column name to follow proper conventions! Next, let's create some visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the target\n",
    "sns.histplot(data=df_class, x='species');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have a completely even distribution of values between the three unique values of our `species` column, or our target vector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data = df_class, y_vars='species');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some outliers in the sepal length and width, however, most of our data is distributed/clustered together."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target vector\n",
    "target = 'species'\n",
    "\n",
    "# Separate target vector from feature matrix\n",
    "X = df_class.drop(columns=target)\n",
    "y = df_class[target]\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\n",
    "\n",
    "# Assert statement to check split\n",
    "assert len(x_train) + len(x_test) == len(X), 'Data did not properly split'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Establish a baseline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from our above example how we establish the baseline for a classification problem. Instead of using the mean like we do in regression, we take the majority class from our target vector instead. We will take the value of this as a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a baseline using value_counts and max\n",
    "cl_base = round(max(df_class.species.value_counts(normalize=True)) *100,2)\n",
    "print(f'Our baseline model can predict our majority class, Iris-setosa, {cl_base}% of the time!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Model Building"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build our models! The key to remember here is that we are building models for a classification task. Because of this, we will want to ensure that we only use the classification versions of our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Pipeline:\n",
    "lr_pipe = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    OneHotEncoder(use_cat_names=True),\n",
    "    LogisticRegression(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "lr_pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Pipeline:\n",
    "dt_pipe = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    OrdinalEncoder(),\n",
    "    DecisionTreeClassifier(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "dt_pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Pipeline:\n",
    "rf_pipe = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    OrdinalEncoder(),\n",
    "    RandomForestClassifier(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "rf_pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Pipeline:\n",
    "xg_pipe = make_pipeline(\n",
    "    SimpleImputer(),\n",
    "    OrdinalEncoder(),\n",
    "    XGBClassifier(objective='binary:logistic')\n",
    ")\n",
    "\n",
    "# Fit the pipeline\n",
    "xg_pipe.fit(x_train,LabelEncoder().fit_transform(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Evaluate and Check Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression:')\n",
    "print(f'Training Accuracy: {round(lr_pipe.score(x_train, y_train),2)*100}')\n",
    "print(f'Testing Accuracy: {round(lr_pipe.score(x_test, y_test),2)*100}')\n",
    "print('--------------------------')\n",
    "print('Decision Tree Classifier:')\n",
    "print(f'Training Accuracy: {round(dt_pipe.score(x_train, y_train), 2)*100}')\n",
    "print(f'Testing Accuracy: {round(dt_pipe.score(x_test, y_test), 2)*100}')\n",
    "print('--------------------------')\n",
    "print('Random Forest Classifier:')\n",
    "print(f'Training Accuracy: {round(rf_pipe.score(x_train, y_train), 2)*100}')\n",
    "print(f'Testing Accuracy: {round(rf_pipe.score(x_test, y_test), 2) * 100}')\n",
    "print('--------------------------')\n",
    "print('XGBoost Classifier:')\n",
    "print(f'Training Accuracy: {round(xg_pipe.score(x_train, y_train), 2)*100}')\n",
    "print(f'Testing Accuracy: {round(xg_pipe.score(x_test, y_test), 2) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression:\")\n",
    "print(f\"Training ROC AUC Score: {round(roc_auc_score(y_train, lr_pipe.predict_proba(x_train), multi_class='ovr'),2)*100}\")\n",
    "print(f'Testing ROC AUC Score: {round(roc_auc_score(y_test, lr_pipe.predict_proba(x_test), multi_class=\"ovr\"),2)*100}')\n",
    "print('------------------------------------')\n",
    "print('Decision Tree Classifier:')\n",
    "print(f\"Training ROC AUC Score: {round(roc_auc_score(y_train, dt_pipe.predict_proba(x_train), multi_class='ovr'),2)*100}\")\n",
    "print(f'Testing ROC AUC Score: {round(roc_auc_score(y_test, dt_pipe.predict_proba(x_test), multi_class=\"ovr\"),2)*100}')\n",
    "print('------------------------------------')\n",
    "print('Random Forest Classifier:')\n",
    "print(f\"Training ROC AUC Score: {round(roc_auc_score(y_train, rf_pipe.predict_proba(x_train), multi_class='ovr'),2)*100}\")\n",
    "print(f'Testing ROC AUC Score: {round(roc_auc_score(y_test, rf_pipe.predict_proba(x_test), multi_class=\"ovr\"),2)*100}')\n",
    "print('------------------------------------')\n",
    "print('XGBoost Classifier:')\n",
    "print(f\"Training ROC AUC Score: {round(roc_auc_score(y_train, xg_pipe.predict_proba(x_train), multi_class='ovr'),2)*100}\")\n",
    "print(f'Testing ROC AUC Score: {round(roc_auc_score(y_test, xg_pipe.predict_proba(x_test), multi_class=\"ovr\"),2)*100}')\n",
    "print('------------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Communicate Results\n",
    "\n",
    "From our metrics above, we will be choosing the Logistic Regression model to explore the results of. Let's visualize the feature importances of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your horizontal barchart here.\n",
    "coefficients = lr_pipe.named_steps['logisticregression'].coef_\n",
    "features = lr_pipe.feature_names_in_\n",
    "print(features)\n",
    "print(coefficients)\n",
    "# Coeff sliced for first value in array.\n",
    "feat_imp = pd.Series(coefficients[0], index=features).sort_values(key=abs)\n",
    "feat_imp.tail(20).plot(kind='barh')\n",
    "plt.xlabel('Coefficient [$]')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Coefficients for Logistic Regression');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "param_grid = {\n",
    "    'logisticregression__C': np.arange(0.1, 2.0, .2),\n",
    "    # 'logisticregression__solver': ['lbfgs', 'liblinear', 'newton-cg', 'sag', 'saga']\n",
    "}\n",
    "clf = RandomizedSearchCV(lr_pipe, param_grid)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
